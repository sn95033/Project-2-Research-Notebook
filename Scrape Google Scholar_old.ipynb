{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 990\n",
      "16.418047910783088\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "#Scrape headlines and summaries from CNN\n",
    "base_url = 'https://scholar.google.com/scholar?start='\n",
    "\n",
    "article_list = []\n",
    "\n",
    "n=0\n",
    "page = 980\n",
    "i= 0\n",
    "\n",
    "\n",
    "for i in range(1):  \n",
    "\n",
    "    url= base_url + str(page) + '&q=open+cell+metal+foam&hl=en&as_sdt=0,5'\n",
    "    browser.visit(url)\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    articles = soup.find_all(\"div\",class_=\"gs_r gs_or gs_scl\")\n",
    "    n=0\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            stats=articles[n].find('div',class_=\"gs_a\").text.strip('&nbsp')\n",
    "            dash_loc = stats.find('-')\n",
    "\n",
    "            authors = stats[0:dash_loc-2]\n",
    "            stats2=stats[dash_loc+1:]\n",
    "            dash_loc_2=stats2.find('-')\n",
    "\n",
    "            journal_date = stats[dash_loc:(dash_loc+dash_loc_2)]\n",
    "            comma_loc=journal_date.find(',')\n",
    "            journal=journal_date[2:comma_loc]\n",
    "            date=journal_date[comma_loc+2:]\n",
    "            \n",
    "            title=articles[n].find('h3',class_=\"gs_rt\").text.strip('<b>')\n",
    "            pdf=articles[n].find('div',class_=\"gs_fl\").text.strip('')\n",
    "            citations=articles[n].find_all('div',class_=\"gs_fl\")\n",
    "            \n",
    "            if len(citations)==2:\n",
    "                citation=citations[1].text.strip(\"Cited by \").strip(\" versions Library Search\")\n",
    "                citation=citation[:3].strip()\n",
    "            else:\n",
    "                citation=citations[0].text.strip(\"Cited by \").strip(\" versions Library Search\")\n",
    "                citation=citation[:3].strip()\n",
    "                pdf=\"No\"\n",
    "            summary = articles[n].find('div',class_=\"gs_rs\").text.strip('<b>')\n",
    "            summary=summary[0:(len(summary)-2)]\n",
    "            \n",
    "            list_item = {\"Title\":title,\"Authors\":authors,\"Journal\":journal,\"Date\":date,\"Stats\":stats,\"Citations\":citation,\"Full Text Available\":pdf,\"Summary\":summary}\n",
    "            article_list.append(list_item)\n",
    "            \n",
    "        except (AttributeError):\n",
    "            print('Missing Data')\n",
    "        \n",
    "        n=n+1\n",
    "        \n",
    "    page=page+10\n",
    "    i=i+1\n",
    "    print(i, page)\n",
    "    \n",
    "    results = pd.DataFrame(article_list)\n",
    "    \n",
    "    #with open('./Output Data/Articles_6.csv', 'a') as f:\n",
    "        \n",
    "    #    results.to_csv(f, index = False, header=False)\n",
    "    \n",
    "    results.to_csv(\"./Output Data/Articles_7.csv\", index=False, header=False, mode = 'a')\n",
    "    \n",
    "    \n",
    "    # random number generator between -1 and 15\n",
    "    s = np.random.uniform(0.1,15.876,)\n",
    "    s2 = np.random.uniform(95.3,120.2)\n",
    "    \n",
    "    print(i*s + page/s2)\n",
    "    \n",
    "    time.sleep(i*s + page/s2)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "\n",
    "#Store data into a csv as a backup\n",
    "cnn_news = pd.DataFrame(article_list)\n",
    "\n",
    "# cnn_news['Summary'].replace('\\xa0...','',inplace=True)\n",
    "\n",
    "cnn_news.to_csv(\"./Output Data/Articles_7.csv\", index=False, header=True)\n",
    "len(cnn_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
