{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "executable_path = {'executable_path': './chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "#Scrape headlines and summaries from CNN\n",
    "base_url = 'https://scholar.google.com/scholar?start='\n",
    "\n",
    "article_list = []\n",
    "\n",
    "n=0\n",
    "page = 30\n",
    "i= 0\n",
    "\n",
    "#Google scholar has installed some limitations on number of pages that can be scraped at once\n",
    "# This scrape  does 10 results (1 page) in a loop\n",
    "# page is the page number you want to start on\n",
    "# range tells you how many pages to scrape at a time\n",
    "\n",
    "\n",
    "for i in range(2):  \n",
    "\n",
    "    url= base_url + str(page) + '&q=open+cell+metal+foam&hl=en&as_sdt=0,5'\n",
    "    browser.visit(url)\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    articles = soup.find_all(\"div\",class_=\"gs_r gs_or gs_scl\")\n",
    "    n=0\n",
    "\n",
    "    for article in articles:\n",
    "        try:\n",
    "            stats=articles[n].find('div',class_=\"gs_a\").text.strip('&nbsp')\n",
    "            dash_loc = stats.find('-')\n",
    "\n",
    "            authors = stats[0:dash_loc-2]\n",
    "            stats2=stats[dash_loc+1:]\n",
    "            dash_loc_2=stats2.find('-')\n",
    "\n",
    "            journal_date = stats[dash_loc:(dash_loc+dash_loc_2)]\n",
    "            comma_loc=journal_date.find(',')\n",
    "            journal=journal_date[2:comma_loc]\n",
    "            date=journal_date[comma_loc+2:]\n",
    "            \n",
    "            title=articles[n].find('h3',class_=\"gs_rt\").text.strip('<b>')\n",
    "            pdf=articles[n].find('div',class_=\"gs_fl\").text.strip('')\n",
    "            citations=articles[n].find_all('div',class_=\"gs_fl\")\n",
    "            \n",
    "            if len(citations)==2:\n",
    "                citation=citations[1].text.strip(\"Cited by \").strip(\" versions Library Search\")\n",
    "                citation=citation[:3].strip()\n",
    "            else:\n",
    "                citation=citations[0].text.strip(\"Cited by \").strip(\" versions Library Search\")\n",
    "                citation=citation[:3].strip()\n",
    "                pdf=\"No\"\n",
    "            summary = articles[n].find('div',class_=\"gs_rs\").text.strip('<b>')\n",
    "            summary=summary[0:(len(summary)-2)]\n",
    "            \n",
    "            list_item = {\"Title\":title,\"Authors\":authors,\"Journal\":journal,\"Date\":date,\"Stats\":stats,\"Citations\":citation,\"Full Text Available\":pdf,\"Summary\":summary}\n",
    "            article_list.append(list_item)\n",
    "            \n",
    "        except (AttributeError):\n",
    "            print('Missing Data')\n",
    "        \n",
    "        n=n+1\n",
    "        \n",
    "    page=page+10\n",
    "    i=i+1\n",
    "    print(i)\n",
    "    \n",
    "    time.sleep(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Store data into a csv as a backup\n",
    "cnn_news = pd.DataFrame(article_list)\n",
    "\n",
    "# cnn_news['Summary'].replace('\\xa0...','',inplace=True)\n",
    "\n",
    "cnn_news.to_csv(\"./Output Data/Articles_3.csv\", index=False, header=True)\n",
    "cnn_news.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7496484322986534"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.random.uniform(-1,2,)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "page = 10\n",
    "print(i,page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
